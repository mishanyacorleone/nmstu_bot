# from sentence_transformers import SentenceTransformer
# import numpy as np
# import onnxruntime as ort
#
#
# onnx_model_path = 'paraphrase-multilingual-MiniLM-L12-v2/onnx/model_O3.onnx'
#
# session = ort.InferenceSession(onnx_model_path, providers=['CPUExecutionProvider'])
#
# model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
#
# sentences = ['–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –ø–æ—Å—Ç—É–ø–∏—Ç—å –≤ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç?', '–ö–∞–∫–∏–µ –µ—Å—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–∏–µ–º–∞?']
#
# inputs = model.tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')
# input_ids = inputs['input_ids'].cpu().numpy().astype(np.int64)
# attention_mask = inputs['attention_mask'].cpu().numpy().astype(np.int64)
# token_type_ids = inputs["token_type_ids"].cpu().numpy().astype(np.int64)
#
# outputs = session.run(None, {
#     'input_ids': input_ids,
#     'attention_mask': attention_mask,
#     'token_type_ids': token_type_ids
# })
#
# embeddings = np.array(outputs[0])
#
# sentence_embeddings = np.mean(embeddings, axis=1)
#
# print(f'–§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {sentence_embeddings.shape}')
# print(f'–ü—Ä–∏–º–µ—Ä —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω–æ–≥–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {sentence_embeddings[0][:5]}')

'''
–í–æ–ø—Ä–æ—Å –¥–ª—è —Ä–∞–∑–¥—É–º—å—è. –ù–µ–æ–±—Ö–æ–¥–∏–º–æ, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª–∞ –¥–æ–∫—É–º–µ–Ω—Ç —Ç–æ–ª—å–∫–æ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –æ–Ω–∞ —É–≤–µ—Ä–µ–Ω–∞, —á—Ç–æ –≤ –¥–∞–Ω–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ
–µ—Å—Ç—å –æ—Ç–≤–µ—Ç –Ω–∞ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å. –ï—Å–ª–∏ –æ–Ω–∞ –Ω–µ —É–≤–µ—Ä–µ–Ω–∞, —Ç–æ –¥–æ–ª–∂–Ω–∞ —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ –∏–ª–∏ –∂–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç
—Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ. –ù–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å "–ù–∞ –∫–∞–∫–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–Ω–µ –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ—Å—Ç—É–ø–∏—Ç—å, –µ—Å–ª–∏ —è —Å–¥–∞–≤–∞–ª [–ø–µ—Ä–µ—á–µ–Ω—å –ø—Ä–µ–¥–º–µ—Ç–æ–≤],
—Ç–æ –æ–Ω–∞ –Ω–µ –¥–æ–ª–∂–Ω–∞ –∏—Å–∫–∞—Ç—å —ç—Ç–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, –∞ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ.
'''


from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import re

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = SentenceTransformer("../sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")


# 2. –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏
def load_text(file_path, chunk_size=500):
    with open(file_path, "r", encoding="utf-8") as f:
        text = f.read()
    # chunks = text.split('\n')
    # chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
    pattern = r"\d+\..*?(?=\n\d+\.|\Z)"
    matches = re.findall(pattern, text, re.DOTALL)

    return matches


text_chunks = load_text("../data/pravila_priema_bsm_na_2025.txt")

# 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
embeddings = model.encode(text_chunks, convert_to_numpy=True)

# 4. –°–æ–∑–¥–∞–Ω–∏–µ FAISS-–∏–Ω–¥–µ–∫—Å–∞
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)  # L2-–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫
index.add(embeddings)


# 5. –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞
def search(query, top_k=3):
    query_embedding = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_embedding, top_k)
    results = [text_chunks[i] for i in indices[0]]
    return results


# 6. –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
query = "–ö–æ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ø—Ä–∏–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤?"
results = search(query)

print("üîç –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:")
for res in results:
    print(res, "\n---")

# from datasets import load_dataset, Dataset
# from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
# from transformers import BitsAndBytesConfig
# # from sklearn.model_selection import train_test_split
# from peft import get_peft_model, LoraConfig, TaskType, PeftModel
# from accelerate.utils.memory import clear_device_cache
# import torch
#
# clear_device_cache()
#
# # dataset = load_dataset('json', data_files='data.json')
#
# # print(dataset['train'][0])
#
# MODEL_NAME = r'RefalMachine_ruadapt_qwen2.5_3B_ext_u48_instruct'
#
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#
# quantization_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_use_double_quant=True,
#     bnb_4bit_quant_type='nf4',
#     bnb_4bit_compute_dtype=torch.float16,
# )
#
# tokenizer = AutoTokenizer.from_pretrained(
#     MODEL_NAME,
#     # use_fast=True,
#     local_files_only=True,
#     # legacy=False
# )
#
# model = AutoModelForCausalLM.from_pretrained(
#     MODEL_NAME,
#     quantization_config=quantization_config,
#     local_files_only=True
# ).to(device)
#
# model = PeftModel.from_pretrained(model, 'results/checkpoint-14940')
#
# prompt = f'<system>–¢—ã - –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç —Å –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ–º –≤ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç.' \
#          f'–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –ï—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞, –Ω–∞–ø–∏—à–∏ "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"</system>' \
#          f'<rules>–ü—Ä–∞–≤–∏–ª–∞:' \
#          f'1. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.' \
#          f'2. –ù–µ –≤–≤–æ–¥–∏ –≤ –∑–∞–±–ª—É–∂–¥–µ–Ω–∏–µ.' \
#          f'3. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–¥–∞–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å. ' \
#          f'4. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ. ' \
#          f'5. –û—Ç–≤–µ—á–∞–π —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏, –∫–∞–∫ —Ç—ã –±—ã –æ—Ç–≤–µ—á–∞–ª —à–∫–æ–ª—å–Ω–∏–∫—É. </rules>' \
#          f'' \
#          f'<question>–í–æ–ø—Ä–æ—Å: {query}</question>' \
#          f'<document>–î–æ–∫—É–º–µ–Ω—Ç: {" ".join(results)}</document>' \
#          f'–û—Ç–≤–µ—Ç (–±–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–∞):'
#
# inputs = tokenizer(prompt, return_tensors="pt").to(device)
#
# output = model.generate(**inputs,
#                         max_new_tokens=512,
#                         num_beams=5,
#                         do_sample=True,
#                         top_p=0.85,
#                         temperature=0.9,
#                         early_stopping=True,
#                         repetition_penalty=2.0,
#                         # eos_token_id=tokenizer.eos_token_id
#                         )
# print(type(inputs))
# print(inputs)
#
# print(tokenizer.decode(output[:, inputs['input_ids'].shape[1]:][0], skip_special_tokens=True))