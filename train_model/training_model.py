from datasets import load_dataset, Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from transformers import BitsAndBytesConfig
import bitsandbytes as bnb
from peft import get_peft_model, LoraConfig, TaskType
import torch

# üìå –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
dataset = load_dataset('json', data_files='answer-response.json')

MODEL_NAME = r'RefalMachine_ruadapt_qwen2.5_3B_ext_u48_instruct'

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# üìå –ù–∞—Å—Ç—Ä–æ–π–∫–∞ 4-–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype=torch.float16,
)

# üìå –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    local_files_only=True,
)

# üìå –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=quantization_config,
    local_files_only=True
).to(device)

# # üî• –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –í–°–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
# for param in model.parameters():
#     param.requires_grad = False
#
# # üî• –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 —Å–ª–æ—è
# for layer in model.model.layers[-4:]:
#     for param in layer.parameters():
#         param.data = param.data.to(torch.float16)  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ float16
#         param.requires_grad = True

# üìå LoRA-–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=16,
    lora_alpha=64,
    lora_dropout=0.05,
    target_modules=['q_proj', 'v_proj']
)

# üìå –ü–æ–¥–∫–ª—é—á–∞–µ–º LoRA –∫ –º–æ–¥–µ–ª–∏
model = get_peft_model(model, peft_config)


# üìå –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
def tokenize_function(batch):
    texts = [p + " " + r for p, r in zip(batch["prompt"], batch["response"])]
    encodings = tokenizer(texts, truncation=True, padding="max_length", max_length=512)
    encodings['labels'] = encodings['input_ids']
    return encodings


# üìå –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# üìå –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
dataset_split = tokenized_datasets['train'].train_test_split(test_size=0.1)
train_dataset = dataset_split['train']
eval_dataset = dataset_split['test']

# üìå –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    fp16=True,
    output_dir="./results",
    gradient_checkpointing=False,
    evaluation_strategy="steps",
    eval_steps=1000,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    learning_rate=5e-5,
    num_train_epochs=3,
)

# üìå Trainer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

# üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
trainer.train()

# üìå –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
model.save_pretrained('nmstuModal-based-qwen2.5-3B-v0.2')
tokenizer.save_pretrained('nmstuModal-based-qwen2.5-3B-v0.2')

# üìå –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
prompt = '/start'
inputs = tokenizer(prompt, return_tensors="pt").to(device)

output = model.generate(**inputs,
                        max_length=128,
                        do_sample=True,
                        early_stopping=True
                        )

print(tokenizer.decode(output[0], skip_special_tokens=True))

